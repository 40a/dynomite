{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Dynomite\n\n\nWelcome to the Dynomite wiki!\n\n\nDynomite is a generic dynamo implementation that can be used with many different key-value pair storage engines. Currently these include Redis and Memcached. Dynomite supports multi-datacenter replication and is designed for high availability.\n\n\nThe ultimate goal with Dynomite is to be able to implement high availability and cross-datacenter replication on storage engines that do not inherently provide that functionality. The implementation is efficient, not complex (few moving parts), and highly performant.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-dynomite", 
            "text": "Welcome to the Dynomite wiki!  Dynomite is a generic dynamo implementation that can be used with many different key-value pair storage engines. Currently these include Redis and Memcached. Dynomite supports multi-datacenter replication and is designed for high availability.  The ultimate goal with Dynomite is to be able to implement high availability and cross-datacenter replication on storage engines that do not inherently provide that functionality. The implementation is efficient, not complex (few moving parts), and highly performant.", 
            "title": "Welcome to Dynomite"
        }, 
        {
            "location": "/1.0 Home/", 
            "text": "Welcome to the Dynomite wiki!  \n\n\nDynomite is a generic \ndynamo\n implementation that can be used with many different key-value pair storage engines. Currently these include \nRedis\n and \nMemcached\n.  Dynomite supports multi-datacenter replication and is designed for high availability.\n\n\nThe ultimate goal with Dynomite is to be able to implement high availability and cross-datacenter replication on storage engines that do not inherently provide that functionality. The implementation is efficient, not complex (few moving parts), and highly performant.", 
            "title": "1.0 Home"
        }, 
        {
            "location": "/2.0 Architecture/", 
            "text": "Architecture Overview\n\n\nIn the open source world, there are various single-server datastore solutions, e.g. Memcached, Redis, BerkeleyDb, LevelDb, Mysql (datastore).  The availability story for these single-server datastores usually ends up being a master-slave setup. Once traffic demands overrun this setup, the next logical progression is to introduce sharding.  Most would agree that it is non trivial to operate this kind of a setup. Furthermore, managing data from different shards is also a challenge for application developers.\n\n\nIn the age of high scalability and big data, Dynomite\u2019s design goal is to turn those single-server datastore solutions into peer-to-peer, linearly scalable, clustered systems while still preserving the native client/server protocols of the datastores, e.g., Redis protocol.\n\n\nDynomite and the target storage engine run on the same node. Clients connect to Dynomite, and requests are proxied to either the storage engine on the same node or to Dynomite processes running on other nodes.   \n\n\n[[images/dynomite-architecture.png]]  \n\n\nAs the request goes through a Dynomite node, the data gets replicated and eventually stored in the target storage.  The data can then be read back either through Dynomite or directly from the underlying storage\u2019s API.", 
            "title": "2.0 Architecture"
        }, 
        {
            "location": "/2.0 Architecture/#architecture-overview", 
            "text": "In the open source world, there are various single-server datastore solutions, e.g. Memcached, Redis, BerkeleyDb, LevelDb, Mysql (datastore).  The availability story for these single-server datastores usually ends up being a master-slave setup. Once traffic demands overrun this setup, the next logical progression is to introduce sharding.  Most would agree that it is non trivial to operate this kind of a setup. Furthermore, managing data from different shards is also a challenge for application developers.  In the age of high scalability and big data, Dynomite\u2019s design goal is to turn those single-server datastore solutions into peer-to-peer, linearly scalable, clustered systems while still preserving the native client/server protocols of the datastores, e.g., Redis protocol.  Dynomite and the target storage engine run on the same node. Clients connect to Dynomite, and requests are proxied to either the storage engine on the same node or to Dynomite processes running on other nodes.     [[images/dynomite-architecture.png]]    As the request goes through a Dynomite node, the data gets replicated and eventually stored in the target storage.  The data can then be read back either through Dynomite or directly from the underlying storage\u2019s API.", 
            "title": "Architecture Overview"
        }, 
        {
            "location": "/2.1 Topology/", 
            "text": "A Dynomite cluster consists of multiple data centers (dc). A datacenter  is a group of racks and rack is a group of nodes. Each rack consists of the entire dataset, which is partitioned across multiple nodes in that rack. Hence multiple racks enable higher availability for data. Each node in a rack has a unique token, which helps to identify which dataset it owns. \n\n\n\n\nEach Dynomite node (e.g., a1 or b1 or c1)  has a Dynomite process co-located with the datastore server, which acts as a proxy, traffic router, coordinator and gossiper. In the context of the Dynamo paper, Dynomite is the Dynamo layer with additional support for pluggable datastore proxy, with an effort to preserve the native datastore protocol as much as possible.  \n\n\nA datastore can be either a volatile datastore such as Memcached or Redis, or persistent datastore such as Mysql, BerkeleyDb or LevelDb.  Our current open sourced Dynomite offering supports Redis and Memcached.", 
            "title": "2.1 Topology"
        }, 
        {
            "location": "/2.2 Replication/", 
            "text": "A client can connect to any node on a Dynomite cluster when sending write traffic.  If the Dynomite node happens to own the data based on its token, then the data is written to the local datastore server process and asynchronously replicated to other racks in the cluster across all data centers.  If the node does not own the data, it acts as a coordinator and sends the write to the node owning the data in the same rack. It also replicates the writes to the corresponding nodes in other racks and DCs.\n\n\nThe diagram below shows an example where data is being written to the cluster by the client. It belongs on nodes a2,b2,c2 and d2 as per the partitioning scheme. The request is sent to a1 which acts as the coordinator and sends the request to the appropriate datastore nodes.\n\n\n[[images/replication1.png]]\n\n\nDynomite features a regular, consistent hash ring. Replication is asymmetric. When a key is hashed to the ring, its owner is the node proceeding it in the ring. As shown in the graphic below, key 30 belongs to node 1 and key 200 belongs to node 4.    \n\n\n[[images/paper-image00.png]]  \n\n\nLocal writes with the Dyno client employ \ntoken aware\n load balancing. The Dyno client is aware of the cluster topology of Dynomite within the same region, and hence can write directly to a specific node using consistent hashing.  \n\n\n[[images/paper-image03.png]]  \n\n\nBelow, the Dyno client does a local write only (i.e local region) and the dynomite co-ordinator know its corresponding replica in the remote region and forwards on the write.\n\n\n[[images/paper-image02.png]]  \n\n\nAsymmetric replication looks slightly different. At some point in the future we could have another region where the capacity is different from the current regions. For example, assume that we have m2.4xl instances in us-east-1 and we have a 6 node token ring in vus-east-1. Then assume that in eu-west-1 we have only m2.2xl instances, hence we have a 12 node token ring in eu-west-1. \n\n\nIn this scenario, key 30 goes to node  1 in us-east-1 but it goes to node 2 in eu-west-1. The Dyno client in each region is aware of the cluster topology and Dynomite is aware of all the topologies for remote regions (via gossip). By design, the client and server and respectively route the write to the correct set of nodes in both regions.  \n\n\n[[images/paper-image01.png]]", 
            "title": "2.2 Replication"
        }, 
        {
            "location": "/2.3 Consistency/", 
            "text": "Dynomite (ver \n0.5.3) extends eventual consistency to tunable consistency in the local region. The consistency level specifies how many replicas must respond to a write/read request before returning data to the client application. Read and write consistency can be configured to manage availability versus data accuracy. Consistency can be configured for read or write operations separately (cluster-wide). There are two configurations.\n\n\nDC_ONE\n:\nReads and writes are propagated synchronously only to the node in the local rack (Availability Zone) and asynchronously replicated to other Racks and regions.\n\n\nDC_QUORUM\n:\nReads and writes are propagated synchronously to quorum number of nodes in the local data center and asynchronously to the rest. The DC_QUORUM configuration writes to the number of nodes that make up a quorum. A quorum is calculated, and then rounded down to a whole number. If all responses are different the first response that the co-ordinator received is returned.\n\n\nDC_SAFE_QUORUM\n:\nSimilarly to DC_QUORUM, but the operation succeeds \nonly\n if the read/write succeeded on a quorum number of nodes and the data checksum \nmatches\n. If the quorum has not been achieved then an error response is generated by Dynomite.\n\n\nConfiguration\n\n\nThere are two ways to configure consistency. The first is in the YAML file and it is picked up on Dynomite's start. The entries in the YAML must appear as follows (the values are case insensitive):\n\n\nread_consistency: dc_quorum\nwrite_consistency: dc_quorum\n\n\n\nThe second is during run time through the admin port (default 22222) on all nodes: \n\n\n$ curl http://localhost:22222/set_consistency/read/dc_quorum\n$ curl http://localhost:22222/set_consistency/write/dc_quorum\n\n\n\nTo check the node level consistency:\n\n\n$ curl http://localhost:22222/get_consistency    \nRead Consistency: DC_QUORUM\nWrite Consistency: DC_QUORUM\n\n\n\nDC_SAFE_QUORUM vs DC_QUORUM\n\n\nThe difference between the two really comes in picture when the query succeeded on quorum nodes but the responses were not the same. If you are very sensitive to reads and writes getting propagated well, use DC_SAFE_QUORUM option. DC_QUORUM gives looser guarantees on quorum in case data mismatch. DC_SAFE_QUORUM will give you error on which you can retry or attempt to repair the data yourself. (straight forward for basic key value). But they both are sensitive to errors.", 
            "title": "2.3 Consistency"
        }, 
        {
            "location": "/2.3 Consistency/#configuration", 
            "text": "There are two ways to configure consistency. The first is in the YAML file and it is picked up on Dynomite's start. The entries in the YAML must appear as follows (the values are case insensitive):  read_consistency: dc_quorum\nwrite_consistency: dc_quorum  The second is during run time through the admin port (default 22222) on all nodes:   $ curl http://localhost:22222/set_consistency/read/dc_quorum\n$ curl http://localhost:22222/set_consistency/write/dc_quorum  To check the node level consistency:  $ curl http://localhost:22222/get_consistency    \nRead Consistency: DC_QUORUM\nWrite Consistency: DC_QUORUM", 
            "title": "Configuration"
        }, 
        {
            "location": "/2.3 Consistency/#dc_safe_quorum-vs-dc_quorum", 
            "text": "The difference between the two really comes in picture when the query succeeded on quorum nodes but the responses were not the same. If you are very sensitive to reads and writes getting propagated well, use DC_SAFE_QUORUM option. DC_QUORUM gives looser guarantees on quorum in case data mismatch. DC_SAFE_QUORUM will give you error on which you can retry or attempt to repair the data yourself. (straight forward for basic key value). But they both are sensitive to errors.", 
            "title": "DC_SAFE_QUORUM vs DC_QUORUM"
        }, 
        {
            "location": "/2.4 Membership-protocols/", 
            "text": "Dynomite supports 2 methods to maintain a node's membership status:\n\n\n1-Centralized membership system\n\n\nEach dynomite node will periodically reach out to a local service (a proxy to the centralized service) to obtain the cluster members.  For more information, please look at this code:\n   https://github.com/Netflix/dynomite/blob/master/src/seedsprovider/dyn_florida.c#L13\n\n\nYou can also check the \ntoken management\n section by Dynomite-manager.\n\n\n2-Gossip\n\n\nGossip is disabled by default and can be turned on by providing \"-g\" flag on the command line.\n  Each node maintains a data structure on the rest of the cluster topology and periodically pick a random node to spread out its data.  Note that in using this you still need to provide a seed list for each node.\n  https://www.cs.cornell.edu/~asdas/research/dsn02-swim.pdf", 
            "title": "2.4 Membership protocols"
        }, 
        {
            "location": "/2.4 Membership-protocols/#1-centralized-membership-system", 
            "text": "Each dynomite node will periodically reach out to a local service (a proxy to the centralized service) to obtain the cluster members.  For more information, please look at this code:\n   https://github.com/Netflix/dynomite/blob/master/src/seedsprovider/dyn_florida.c#L13  You can also check the  token management  section by Dynomite-manager.", 
            "title": "1-Centralized membership system"
        }, 
        {
            "location": "/2.4 Membership-protocols/#2-gossip", 
            "text": "Gossip is disabled by default and can be turned on by providing \"-g\" flag on the command line.\n  Each node maintains a data structure on the rest of the cluster topology and periodically pick a random node to spread out its data.  Note that in using this you still need to provide a seed list for each node.\n  https://www.cs.cornell.edu/~asdas/research/dsn02-swim.pdf", 
            "title": "2-Gossip"
        }, 
        {
            "location": "/2.5 Warm-up/", 
            "text": "For more information about the warm up, please see \nDynomite-manager's Cold Bootstraping\n.", 
            "title": "2.5 Warm up"
        }, 
        {
            "location": "/2.6 Token-Management/", 
            "text": "Dynomite is based on the Dynamo protocol where nodes serve keys that belong inside a token range. Differently from Cassandra, each Rack contains the full ring. Hence in RF=3 configuration the full ring is deployed in 3 Racks, whereas in RF=2 the full ring is deployed in 2 racks.\n\n\nTo create tokens you can use this script to generate \nYAMLS\n\n\nExemplar Configuration YAML files can be found in the \nconf directory", 
            "title": "2.6 Token Management"
        }, 
        {
            "location": "/2.7 Use-Cases/", 
            "text": "Use Cases...  \n\n\nUse Case 1\n\n\nUse Case 2", 
            "title": "2.7 Use Cases"
        }, 
        {
            "location": "/2.7 Use-Cases/#use-case-1", 
            "text": "", 
            "title": "Use Case 1"
        }, 
        {
            "location": "/2.7 Use-Cases/#use-case-2", 
            "text": "", 
            "title": "Use Case 2"
        }, 
        {
            "location": "/3.0 Performance/", 
            "text": "Please click on our benchmarking tests for more informations", 
            "title": "3.0 Performance"
        }, 
        {
            "location": "/3.1 Basic-benchmarking/", 
            "text": "Details\n\n Server instance type was r3.xlarge. These instances are well suited for Dynomite workloads - i.e good balance between memory and network\n\n Replication factor was set to 3. We did this by deploying Dynomite in 3 zones in us-east-1 and each zone had the same no of servers and hence the same no of tokens. \n\n Client fleet used instance type as m2.2xls which is also typical for an application here at Netflix. \n\n Demo application used a simple workload of just key value pairs for read and writes i.e the Redis GET and SET api. \n\n Payload size was chosen to be 1K \n\n We maintained an 80% - 20% ratio between reads and writes which is also typical of many use cases here at Netflix.\n\n\nStage 1 -   6 node cluster\n\n\nWe setup a Dynomite cluster of 6 nodes (i.e 2 per zone). \nWe observed throughput of about 80k per second across the client fleet:\n[[/images/linear_scale/linear_scale_6_rps.png|align=center]]\n\n\nwhile keeping the avg latency ~1 ms:\n[[/images/linear_scale/linear_scale_6_p50.png|align=center]]\n\n\nand 99 percentile latency in the single digit ms range:\n[[/images/linear_scale/linear_scale_6_p99.png|align=center]] \n\n\nStage 2 -   12 node cluster\n\n\nWe then doubled the size of the Dynomite cluster from 6 nodes to 12 nodes. \nWe also simultaneously doubled our client fleet to add more load on the cluster. \n\n\nAs expected client fleet throughput went up by 100%\n[[/images/linear_scale/linear_scale_12_rps.png|align=center]]\n\n\nwhile still keeping avg and 99 percentile latencies in check\n[[/images/linear_scale/linear_scale_12_p50.png|align=center]]\n[[/images/linear_scale/linear_scale_12_p99.png|align=center]] \n\n\nStage 3 -   24 node cluster\n\n\nWe then went for one more double for the server and client fleet and throughput went up by 100% once again,\nwhile latencies remained the same. \n[[/images/linear_scale/linear_scale_24_rps.png|align=center]]\n[[/images/linear_scale/linear_scale_24_p50.png|align=center]]\n[[/images/linear_scale/linear_scale_24_p99.png|align=center]] \n\n\nReplication Delay\n\n\nIn our initial tests, we measured the time it took for a key/value pair to become available on another region replica by writing 1k key/value pairs to Dynomite in one region, then polling the other region randomly for 20 keys.  The value for each key in this case is just timestamp when the write action started.  The client in the other region then reads back those timestamps and compute the durations.  We repeated this same experiment several times and took the average.  From this we could derive a rough idea of the speed of the replication.  \n\n\nWe expect this latency to remain more or less constant as we add code path optimization as well as enhancements in the replication strategy itself (optimizations will improve speed, features will potentially add latency).  \n\n\nResult:   \n\n\n\n\nFor 5 iterations of this experiment, the average duration for replications was around 85ms.\n(note that the duration numbers are measured at the client layers so the real numbers should be smaller).", 
            "title": "3.1 Basic benchmarking"
        }, 
        {
            "location": "/3.1 Basic-benchmarking/#stage-1-6-node-cluster", 
            "text": "We setup a Dynomite cluster of 6 nodes (i.e 2 per zone). \nWe observed throughput of about 80k per second across the client fleet:\n[[/images/linear_scale/linear_scale_6_rps.png|align=center]]  while keeping the avg latency ~1 ms:\n[[/images/linear_scale/linear_scale_6_p50.png|align=center]]  and 99 percentile latency in the single digit ms range:\n[[/images/linear_scale/linear_scale_6_p99.png|align=center]]", 
            "title": "Stage 1 -   6 node cluster"
        }, 
        {
            "location": "/3.1 Basic-benchmarking/#stage-2-12-node-cluster", 
            "text": "We then doubled the size of the Dynomite cluster from 6 nodes to 12 nodes. \nWe also simultaneously doubled our client fleet to add more load on the cluster.   As expected client fleet throughput went up by 100%\n[[/images/linear_scale/linear_scale_12_rps.png|align=center]]  while still keeping avg and 99 percentile latencies in check\n[[/images/linear_scale/linear_scale_12_p50.png|align=center]]\n[[/images/linear_scale/linear_scale_12_p99.png|align=center]]", 
            "title": "Stage 2 -   12 node cluster"
        }, 
        {
            "location": "/3.1 Basic-benchmarking/#stage-3-24-node-cluster", 
            "text": "We then went for one more double for the server and client fleet and throughput went up by 100% once again,\nwhile latencies remained the same. \n[[/images/linear_scale/linear_scale_24_rps.png|align=center]]\n[[/images/linear_scale/linear_scale_24_p50.png|align=center]]\n[[/images/linear_scale/linear_scale_24_p99.png|align=center]]", 
            "title": "Stage 3 -   24 node cluster"
        }, 
        {
            "location": "/3.1 Basic-benchmarking/#replication-delay", 
            "text": "In our initial tests, we measured the time it took for a key/value pair to become available on another region replica by writing 1k key/value pairs to Dynomite in one region, then polling the other region randomly for 20 keys.  The value for each key in this case is just timestamp when the write action started.  The client in the other region then reads back those timestamps and compute the durations.  We repeated this same experiment several times and took the average.  From this we could derive a rough idea of the speed of the replication.    We expect this latency to remain more or less constant as we add code path optimization as well as enhancements in the replication strategy itself (optimizations will improve speed, features will potentially add latency).    Result:      For 5 iterations of this experiment, the average duration for replications was around 85ms.\n(note that the duration numbers are measured at the client layers so the real numbers should be smaller).", 
            "title": "Replication Delay"
        }, 
        {
            "location": "/3.2 Dynomite-with-Redis-on-AWS---Benchmarks/", 
            "text": "About a year ago the Cloud Database Engineering (CDE) team published a post \nIntroducing Dynomite\n. Dynomite is a proxy layer that provides sharding and replication and can turn existing non-distributed datastores into a fully distributed system with multi-region replication. One of its core features is the ability to scale a data store linearly to meet rapidly increasing traffic demands. Dynomite also provides high availability, and was designed and built to support \nActive-Active Multi-Regional Resiliency\n. \nDynomite, with \nRedis\n is now utilized as a production system within Netflix. This post is the first part of a series that pragmatically examines Dynomite's use cases and features. In this post, we will show performance results using Amazon Web Services (AWS) with and without the recently added consistency feature. \n\n\nDynomite Consistency\n\n\nDynomite extends eventual consistency to \ntunable consistency\n in the local region. The consistency level specifies how many replicas must respond to a write/read request before returning data to the client application. Read and write consistency can be configured to manage availability versus data accuracy. Consistency can be configured for read or write operations separately (cluster-wide). There are two configurations:\n\n\nDC_ONE\n:\nReads and writes are propagated synchronously only to the node in the local rack (Availability Zone) and asynchronously replicated to other Racks and regions.\n\n\nDC_QUORUM\n:\nReads and writes are propagated synchronously to quorum number of nodes in the local data center and asynchronously to the rest. The DC_QUORUM configuration writes to the number of nodes that make up a quorum. A quorum is calculated, and then rounded up to a whole number. The operation succeeds if the read/write succeeded on a quorum number of nodes.\n\n\nTest Setup\n\n\nFor the workload generator, we used an internal Netflix tool called Pappy. Pappy is well integrated with other Netflix OSS services such as (Archaius for fast properties, Servo for metrics, and Eureka for discovery). However, any other other distributed load generator with Redis client plugin can be used to replicate the results. Pappy has support for modules, and one of them is Dyno Java client.\n\n\nDyno client uses topology aware load balancing (Token Aware) to directly connect to a Dynomite coordinator node that is the owner of the specified data. Dyno also uses zone awareness to send traffic to Dynomite nodes in the local ASG. To get full benefit of a Dynomite cluster a) the Dyno client cluster should be deployed across all ASGs, so all nodes can receive client traffic, and b) the number of client application nodes per ASG must be larger than the corresponding number of Dynomite nodes in the respective ASG so that the cumulative network capacity of the client cluster is at least equal to the corresponding one at the Dynomite layer. \n\n\nDyno also uses connection pooling for persistent connections to reduce the connection churn to the Dynomite nodes. However, in performance benchmarks tuning Dyno can tricky as the workload generator make become the bottleneck due to thread contention. In our benchmark, we observed the delay metrics to pick up a connection from the connection pool that Dyno exposes.\n\n\nClient (workload generator) Cluster\n\n Client: Dyno Java client, using default configuration (token aware + zone aware) \n\n Number of nodes: Equal to the number of Dynomite nodes in each experiment.\n\n Region: us-west-2 (us-west-2a, us-west-2b and us-west-2c)\n\n EC2 instance type: m3.2xlarge (30GB RAM, 8 CPU cores, Network throughput: high)\n\n Platform: EC2-Classic\n\n Data size: 1024 Bytes\n\n Number of Keys: 1M random keys\n\n Demo application used a simple workload of just key value pairs for read and writes i.e the Redis GET and SET api.\n* Read/Write ratio: 80:20 (the OPS was variable per test, but the ratio was kept 80:20)\nNumber of readers/writers: 80:20 ratio of reader to writer threads. 32 readers/8 writers  per Dynomite Node. We performed some experiments varying the number of readers and writers and found that in the context of our experiments, 32 readers and 8 writes per dynomite node gave the best throughput latency tradeoff.\n\n\nDynomite Cluster\n\n Dynomite: Dynomite 0.5.6\n\n Data store: Redis 2.8.9\n\n Operating system: Ubuntu 14.04.2 LTS\n\n Number of nodes: 3-48 (doubling every time)\n\n Region: us-west-2 (us-west-2a, us-west-2b and us-west-2c)\n\n EC2 instance type: r3.2xlarge (61GB RAM, 8 CPU cores, Network throughput: high)\n* Platform: EC2-Classic\n\n\nThe test was performed in a single region with 3 availability zones. Note that replicating to two other availability zones is not a requirement for Dynomite, but rather a deployment choice for high availability at Netflix. A Dynomite cluster of 3 nodes means that there was 1 node per availability zone. However all three nodes take client traffic as well as replication traffic from peer nodes. Our results were captured using \nAtlas\n. Each experiment was run 3 times, and our results are averaged over based on average of these times. Each run lasted 3h.\n\n\nFor the our benchmarks, we refer to the Dynomite node, as the node that contains the Dynomite layer and Redis. Hence we do not distinguish on whether Dynomite layer or Redis contributes to the average latency. \n\n\nLinear Scale Test with DC_ONE\n\n\n\n\n\n\n\nThe throughput graphs indicate that Dynomite can scale horizontally in terms of throughput. Therefore, it can handle even more traffic by increasing the number of nodes per region.\n\n\nMoreover, on a per node basis with r3.2xlarge nodes, Dynomite can roughly process 33K reads OPS and around 10K write OPS concurrently from the client. This is because the traffic coming to each node is replicated to two other availability zones, therefore dribbling the effective throughput on a per node basis. Note that replicating to two other availability zones is not requirement for Dynomite, but rather a deployment choice at Netflix. For 1KB payload, as the graphs show the main bottleneck is the network (1Gbps EC2 instances). Therefore, switching to 2Gbps r3.4xlarge instances or 10Gbps capable EC2 instances, Dynomite can potentially provide even faster throughput. Note though that 10Gbps optimizations will only be effective when the instances are launched in Amazon's VPC with instance types that can support \nEnhanced Networking using single root I/O virtualization (SR-IOV)\n. \n\n\nThe actual provisioning formula is also affected by the design of the client application. For example, to get full benefit of a Dynomite cluster the number of client nodes on a per availability zone at the client application must be larger than the corresponding ones on Dynomite. This will ensure that the client traffic is evenly distributed across the Dynomite cluster. Moreover, one also has to take into account the thread locking on the client side and the number of connections made to each Dynomite node. These can be observed at the Dyno client by looking into the delay to pick up a connection from the connection pool.\n\n\n\n\nFor the above results, we refer to the Dynomite node, as the node that contains the Dynomite layer and Redis.  Hence we do not distinguish on whether Dynomite layer or Redis contributes to the average latency. The average and median latency values show that Dynomite can provide sub-millisecond latency to the client application. More specifically, Dynomite does not add an extra latency as it scales to higher number of nodes, and therefore higher throughput. Overall, the Dynomite node adds around 20% of the average latency, and the rest of it is a result of the network latency. \n\n\n\n\nAt the 95th percentile Dynomite's latency is 0.4ms and does not increase as we scale the cluster up/down. More specifically, the network is the major reason for the 95th percentile latency, as Dynomite's node effect is \n10%. \n\n\n\n\nIt is evident from the 99th percentile graph where the latency for Dynomite pretty much remains then same while the client side increases indicating the variable nature of the network between the clusters.\n\n\nLinear Scale Test With DC_QUORUM\n\n\nThe test setup was similar to what we used for DC_ONE tests above. Consistency was set to DC_QUORUM for both writes and reads on all Dynomite nodes.\n\n\n\n\n\n\n\nLooking at the graph it is clear that dynomite still scales well as the cluster nodes are increasing. Moreover Dynomite node achieves 15K IOPS per node in our setup, when the cluster spans a single region. \n\n\n\n\nThe average and median latency remains \n3ms even when DC_QUORUM consistency is enabled in Dynomite nodes. Evidently, the average and median latency are slightly higher than the corresponding experiments with DC_ONE. This because the latency includes one to two hops to other availability zones and the corresponding operations performed on those nodes.\n\n\n\nThe 95th percentile at the Dynomite level is less 2.5ms regardless of the traffic sent to the cluster (linear scale), and at the client side it is below 4ms.\n\n\n\n\nAt the 99th Percentile with DC_QUORUM enabled, Dynomite produces less than 4ms of latency. When considering the network from the cluster to the client, the latency remains well below 10ms opening the door for a number of applications that require consistency with low latency across the board of percentiles.\n\n\nPipelining\n\n\nPipelining\n is client side batching; the client sends requests without waiting for a response from a previous request and later reads response for the whole batch. This way, the overall throughput and client side latency can increase. It is different from a transaction where all or none operations succeed. In pipelining, individual operations can succeed or fail. In the following experiments, the Dyno client randomly selected between 3 to 10 operations in one pipeline request. The experiments were performed for both DC_ONE and DC_QUORUM. \n\n\n\n\n\n\nFor comparison reason, we showcase both the non-pipelining and pipelining results. Evidently, pipelining increases the throughput by 20-50%. For a small Dynomite cluster the improvement is larger, but as Dynomite horizontally scales the benefit of pipeline decreases.\n\n\nLatency is really a factor of how many requests are clubbed into one pipeline request so it will vary and will be higher than non pipeline requests.\n\n\nConclusion\n\n\nWe performed the tests to get some more insights about Dynomite using Redis at the data store layer, and how to size our clusters. We could have achieved better results with better instance types both at the client and Dyomite server cluster. For example, adding Dynomite nodes with better network capacity (especially the ones supporting enhanced Networking on Linux Instances in a VPC) could further increase the performance of our clusters.\nAnother way to improve the performance is by using fewer availability zones. In that case,  Dynomite would replicate the data in one more availability zone instead of two more, hence more bandwidth would have been available to client connections. In our experiment we used 3 availability zones in us-west-2, which is a common deployment in most production clusters at Netflix. \n\n\nIn summary, our benchmarks were based on instance types and deployments that are common at Netflix and Dynomite. We presented results that indicate that DC_QUORUM provides better read and write guarantees to the client but with higher latencies and lower throughput. We also showcased how a client can configure Redis Pipeline and benefit from request batching.\n\n\nWe briefly mentioned the availability of higher consistency in this article. In the next article we'll dive deeper into how we implemented higher consistency and how we handle anti-entropy.", 
            "title": "3.2 Dynomite with Redis on AWS   Benchmarks"
        }, 
        {
            "location": "/3.2 Dynomite-with-Redis-on-AWS---Benchmarks/#dynomite-consistency", 
            "text": "Dynomite extends eventual consistency to  tunable consistency  in the local region. The consistency level specifies how many replicas must respond to a write/read request before returning data to the client application. Read and write consistency can be configured to manage availability versus data accuracy. Consistency can be configured for read or write operations separately (cluster-wide). There are two configurations:  DC_ONE :\nReads and writes are propagated synchronously only to the node in the local rack (Availability Zone) and asynchronously replicated to other Racks and regions.  DC_QUORUM :\nReads and writes are propagated synchronously to quorum number of nodes in the local data center and asynchronously to the rest. The DC_QUORUM configuration writes to the number of nodes that make up a quorum. A quorum is calculated, and then rounded up to a whole number. The operation succeeds if the read/write succeeded on a quorum number of nodes.", 
            "title": "Dynomite Consistency"
        }, 
        {
            "location": "/3.2 Dynomite-with-Redis-on-AWS---Benchmarks/#test-setup", 
            "text": "For the workload generator, we used an internal Netflix tool called Pappy. Pappy is well integrated with other Netflix OSS services such as (Archaius for fast properties, Servo for metrics, and Eureka for discovery). However, any other other distributed load generator with Redis client plugin can be used to replicate the results. Pappy has support for modules, and one of them is Dyno Java client.  Dyno client uses topology aware load balancing (Token Aware) to directly connect to a Dynomite coordinator node that is the owner of the specified data. Dyno also uses zone awareness to send traffic to Dynomite nodes in the local ASG. To get full benefit of a Dynomite cluster a) the Dyno client cluster should be deployed across all ASGs, so all nodes can receive client traffic, and b) the number of client application nodes per ASG must be larger than the corresponding number of Dynomite nodes in the respective ASG so that the cumulative network capacity of the client cluster is at least equal to the corresponding one at the Dynomite layer.   Dyno also uses connection pooling for persistent connections to reduce the connection churn to the Dynomite nodes. However, in performance benchmarks tuning Dyno can tricky as the workload generator make become the bottleneck due to thread contention. In our benchmark, we observed the delay metrics to pick up a connection from the connection pool that Dyno exposes.  Client (workload generator) Cluster  Client: Dyno Java client, using default configuration (token aware + zone aware)   Number of nodes: Equal to the number of Dynomite nodes in each experiment.  Region: us-west-2 (us-west-2a, us-west-2b and us-west-2c)  EC2 instance type: m3.2xlarge (30GB RAM, 8 CPU cores, Network throughput: high)  Platform: EC2-Classic  Data size: 1024 Bytes  Number of Keys: 1M random keys  Demo application used a simple workload of just key value pairs for read and writes i.e the Redis GET and SET api.\n* Read/Write ratio: 80:20 (the OPS was variable per test, but the ratio was kept 80:20)\nNumber of readers/writers: 80:20 ratio of reader to writer threads. 32 readers/8 writers  per Dynomite Node. We performed some experiments varying the number of readers and writers and found that in the context of our experiments, 32 readers and 8 writes per dynomite node gave the best throughput latency tradeoff.  Dynomite Cluster  Dynomite: Dynomite 0.5.6  Data store: Redis 2.8.9  Operating system: Ubuntu 14.04.2 LTS  Number of nodes: 3-48 (doubling every time)  Region: us-west-2 (us-west-2a, us-west-2b and us-west-2c)  EC2 instance type: r3.2xlarge (61GB RAM, 8 CPU cores, Network throughput: high)\n* Platform: EC2-Classic  The test was performed in a single region with 3 availability zones. Note that replicating to two other availability zones is not a requirement for Dynomite, but rather a deployment choice for high availability at Netflix. A Dynomite cluster of 3 nodes means that there was 1 node per availability zone. However all three nodes take client traffic as well as replication traffic from peer nodes. Our results were captured using  Atlas . Each experiment was run 3 times, and our results are averaged over based on average of these times. Each run lasted 3h.  For the our benchmarks, we refer to the Dynomite node, as the node that contains the Dynomite layer and Redis. Hence we do not distinguish on whether Dynomite layer or Redis contributes to the average latency.", 
            "title": "Test Setup"
        }, 
        {
            "location": "/3.2 Dynomite-with-Redis-on-AWS---Benchmarks/#linear-scale-test-with-dc_one", 
            "text": "The throughput graphs indicate that Dynomite can scale horizontally in terms of throughput. Therefore, it can handle even more traffic by increasing the number of nodes per region.  Moreover, on a per node basis with r3.2xlarge nodes, Dynomite can roughly process 33K reads OPS and around 10K write OPS concurrently from the client. This is because the traffic coming to each node is replicated to two other availability zones, therefore dribbling the effective throughput on a per node basis. Note that replicating to two other availability zones is not requirement for Dynomite, but rather a deployment choice at Netflix. For 1KB payload, as the graphs show the main bottleneck is the network (1Gbps EC2 instances). Therefore, switching to 2Gbps r3.4xlarge instances or 10Gbps capable EC2 instances, Dynomite can potentially provide even faster throughput. Note though that 10Gbps optimizations will only be effective when the instances are launched in Amazon's VPC with instance types that can support  Enhanced Networking using single root I/O virtualization (SR-IOV) .   The actual provisioning formula is also affected by the design of the client application. For example, to get full benefit of a Dynomite cluster the number of client nodes on a per availability zone at the client application must be larger than the corresponding ones on Dynomite. This will ensure that the client traffic is evenly distributed across the Dynomite cluster. Moreover, one also has to take into account the thread locking on the client side and the number of connections made to each Dynomite node. These can be observed at the Dyno client by looking into the delay to pick up a connection from the connection pool.   For the above results, we refer to the Dynomite node, as the node that contains the Dynomite layer and Redis.  Hence we do not distinguish on whether Dynomite layer or Redis contributes to the average latency. The average and median latency values show that Dynomite can provide sub-millisecond latency to the client application. More specifically, Dynomite does not add an extra latency as it scales to higher number of nodes, and therefore higher throughput. Overall, the Dynomite node adds around 20% of the average latency, and the rest of it is a result of the network latency.    At the 95th percentile Dynomite's latency is 0.4ms and does not increase as we scale the cluster up/down. More specifically, the network is the major reason for the 95th percentile latency, as Dynomite's node effect is  10%.    It is evident from the 99th percentile graph where the latency for Dynomite pretty much remains then same while the client side increases indicating the variable nature of the network between the clusters.", 
            "title": "Linear Scale Test with DC_ONE"
        }, 
        {
            "location": "/3.2 Dynomite-with-Redis-on-AWS---Benchmarks/#linear-scale-test-with-dc_quorum", 
            "text": "The test setup was similar to what we used for DC_ONE tests above. Consistency was set to DC_QUORUM for both writes and reads on all Dynomite nodes.    Looking at the graph it is clear that dynomite still scales well as the cluster nodes are increasing. Moreover Dynomite node achieves 15K IOPS per node in our setup, when the cluster spans a single region.    The average and median latency remains  3ms even when DC_QUORUM consistency is enabled in Dynomite nodes. Evidently, the average and median latency are slightly higher than the corresponding experiments with DC_ONE. This because the latency includes one to two hops to other availability zones and the corresponding operations performed on those nodes.  \nThe 95th percentile at the Dynomite level is less 2.5ms regardless of the traffic sent to the cluster (linear scale), and at the client side it is below 4ms.   At the 99th Percentile with DC_QUORUM enabled, Dynomite produces less than 4ms of latency. When considering the network from the cluster to the client, the latency remains well below 10ms opening the door for a number of applications that require consistency with low latency across the board of percentiles.", 
            "title": "Linear Scale Test With DC_QUORUM"
        }, 
        {
            "location": "/3.2 Dynomite-with-Redis-on-AWS---Benchmarks/#pipelining", 
            "text": "Pipelining  is client side batching; the client sends requests without waiting for a response from a previous request and later reads response for the whole batch. This way, the overall throughput and client side latency can increase. It is different from a transaction where all or none operations succeed. In pipelining, individual operations can succeed or fail. In the following experiments, the Dyno client randomly selected between 3 to 10 operations in one pipeline request. The experiments were performed for both DC_ONE and DC_QUORUM.     For comparison reason, we showcase both the non-pipelining and pipelining results. Evidently, pipelining increases the throughput by 20-50%. For a small Dynomite cluster the improvement is larger, but as Dynomite horizontally scales the benefit of pipeline decreases.  Latency is really a factor of how many requests are clubbed into one pipeline request so it will vary and will be higher than non pipeline requests.", 
            "title": "Pipelining"
        }, 
        {
            "location": "/3.2 Dynomite-with-Redis-on-AWS---Benchmarks/#conclusion", 
            "text": "We performed the tests to get some more insights about Dynomite using Redis at the data store layer, and how to size our clusters. We could have achieved better results with better instance types both at the client and Dyomite server cluster. For example, adding Dynomite nodes with better network capacity (especially the ones supporting enhanced Networking on Linux Instances in a VPC) could further increase the performance of our clusters.\nAnother way to improve the performance is by using fewer availability zones. In that case,  Dynomite would replicate the data in one more availability zone instead of two more, hence more bandwidth would have been available to client connections. In our experiment we used 3 availability zones in us-west-2, which is a common deployment in most production clusters at Netflix.   In summary, our benchmarks were based on instance types and deployments that are common at Netflix and Dynomite. We presented results that indicate that DC_QUORUM provides better read and write guarantees to the client but with higher latencies and lower throughput. We also showcased how a client can configure Redis Pipeline and benefit from request batching.  We briefly mentioned the availability of higher consistency in this article. In the next article we'll dive deeper into how we implemented higher consistency and how we handle anti-entropy.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/4.0 Getting-Started/", 
            "text": "In this setup, we will use Redis with Dynomite.  For Memcached, it is very similar replacing Redis installation step with Memcached installation step.  We will also show you the configurations to simulate a one-node Dynomite cluster and a two-node Dynomite cluster.\n\n\n1. Install Redis\n\n\nPlease follow the instructions on this website to download and install Redis: http://redis.io/\n\n\n2. Build Dynomite\n\n\nPlease follow our \nREADME.md\n to compile and build Dynomite from source. Note that we only test Dynomite on Linux environments so we recommend compiling and building Dynomite on a Linux system.\n\n\n3. Configuration\n\n\nThese examples are for redis.  If you remove redis property, they will become memcached configured files.\n\n\na. One node cluster:\n\n\n   https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_single.yml\n\n\n\nb. Two-nodes cluster in one datacenter with 2 racks\n\n\n   https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_dc1_rack1.yml\n   https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_dc1_rack2.yml\n\n\n\nc. Two-nodes cluster in two datacenters each having one rack\n\n\n   https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_mul_dc1.yml\n   https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_mul_dc2.yml\n\n\n\nd. 6-node cluster in one datacenter with 3 racks, each having 2 nodes\n\n\ndyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '1383429731'\n     rack: rack-v1\n     data_store: 0\n\n\n\n\n\ndyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '1383429731'\n     rack: rack-v0\n     data_store: 0\n\n\n\n\n\ndyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '1383429731'\n     rack: rack-v2\n     data_store: 0\n\n\n\n\n\ndyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '3530913378'\n     rack: rack-v1\n     data_store: 0\n\n\n\n\n\ndyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '3530913378'\n     rack: rack-v0\n     data_store: 0\n\n\n\n\n\ndyn_o_mite:\n     auto_eject_hosts: true\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '3530913378'\n     rack: rack-v2\n     data_store: 0\n\n\n\n4. Start the cluster(s)\n\n\n   Assume you have gone through the build process and produced the binary file \"dynomite\".\n   To run a one node dynomite cluster using the above config, you can run \n      \"dynomite -c dynomite.yml\".\n\n   If you want to start a 2 node cluster with those two yaml files, you can open \n   2 terminals and run:\n     Terminal 1: $ dynomite -c dynomite1.yml\n     Terminal 2: $ dynomite -c dynomite2.yml", 
            "title": "4.0 Getting Started"
        }, 
        {
            "location": "/4.0 Getting-Started/#1-install-redis", 
            "text": "Please follow the instructions on this website to download and install Redis: http://redis.io/", 
            "title": "1. Install Redis"
        }, 
        {
            "location": "/4.0 Getting-Started/#2-build-dynomite", 
            "text": "Please follow our  README.md  to compile and build Dynomite from source. Note that we only test Dynomite on Linux environments so we recommend compiling and building Dynomite on a Linux system.", 
            "title": "2. Build Dynomite"
        }, 
        {
            "location": "/4.0 Getting-Started/#3-configuration", 
            "text": "These examples are for redis.  If you remove redis property, they will become memcached configured files.", 
            "title": "3. Configuration"
        }, 
        {
            "location": "/4.0 Getting-Started/#a-one-node-cluster", 
            "text": "https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_single.yml", 
            "title": "a. One node cluster:"
        }, 
        {
            "location": "/4.0 Getting-Started/#b-two-nodes-cluster-in-one-datacenter-with-2-racks", 
            "text": "https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_dc1_rack1.yml\n   https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_dc1_rack2.yml", 
            "title": "b. Two-nodes cluster in one datacenter with 2 racks"
        }, 
        {
            "location": "/4.0 Getting-Started/#c-two-nodes-cluster-in-two-datacenters-each-having-one-rack", 
            "text": "https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_mul_dc1.yml\n   https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_mul_dc2.yml", 
            "title": "c. Two-nodes cluster in two datacenters each having one rack"
        }, 
        {
            "location": "/4.0 Getting-Started/#d-6-node-cluster-in-one-datacenter-with-3-racks-each-having-2-nodes", 
            "text": "dyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '1383429731'\n     rack: rack-v1\n     data_store: 0   dyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '1383429731'\n     rack: rack-v0\n     data_store: 0   dyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '1383429731'\n     rack: rack-v2\n     data_store: 0   dyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '3530913378'\n     rack: rack-v1\n     data_store: 0   dyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '3530913378'\n     rack: rack-v0\n     data_store: 0   dyn_o_mite:\n     auto_eject_hosts: true\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '3530913378'\n     rack: rack-v2\n     data_store: 0", 
            "title": "d. 6-node cluster in one datacenter with 3 racks, each having 2 nodes"
        }, 
        {
            "location": "/4.0 Getting-Started/#4-start-the-clusters", 
            "text": "Assume you have gone through the build process and produced the binary file \"dynomite\".\n   To run a one node dynomite cluster using the above config, you can run \n      \"dynomite -c dynomite.yml\".\n\n   If you want to start a 2 node cluster with those two yaml files, you can open \n   2 terminals and run:\n     Terminal 1: $ dynomite -c dynomite1.yml\n     Terminal 2: $ dynomite -c dynomite2.yml", 
            "title": "4. Start the cluster(s)"
        }, 
        {
            "location": "/4.1 Setup and Installation/", 
            "text": "In this setup, we will use Redis with Dynomite.  For Memcached, it is very similar replacing Redis installation step with Memcached installation step.  We will also show you the configurations to simulate a one-node Dynomite cluster and a two-node Dynomite cluster.\n\n\n1. Install Redis\n\n\nPlease follow the instructions on this website to download and install Redis: http://redis.io/\n\n\n2. Build Dynomite\n\n\nPlease follow our \nREADME.md\n to compile and build Dynomite from source. Note that we only test Dynomite on Linux environments so we recommend compiling and building Dynomite on a Linux system.\n\n\n3. Configuration\n\n\nThese examples are for redis.  If you remove redis property, they will become memcached configured files.\n\n\na. One node cluster:\n\n\n   https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_single.yml\n\n\n\nb. Two-nodes cluster in one datacenter with 2 racks\n\n\n   https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_dc1_rack1.yml\n   https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_dc1_rack2.yml\n\n\n\nc. Two-nodes cluster in two datacenters each having one rack\n\n\n   https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_mul_dc1.yml\n   https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_mul_dc2.yml\n\n\n\nd. 6-node cluster in one datacenter with 3 racks, each having 2 nodes\n\n\ndyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '1383429731'\n     rack: rack-v1\n     data_store: 0\n\n\n\n\n\ndyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '1383429731'\n     rack: rack-v0\n     data_store: 0\n\n\n\n\n\ndyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '1383429731'\n     rack: rack-v2\n     data_store: 0\n\n\n\n\n\ndyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '3530913378'\n     rack: rack-v1\n     data_store: 0\n\n\n\n\n\ndyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '3530913378'\n     rack: rack-v0\n     data_store: 0\n\n\n\n\n\ndyn_o_mite:\n     auto_eject_hosts: true\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '3530913378'\n     rack: rack-v2\n     data_store: 0\n\n\n\n4. Start the cluster(s)\n\n\n   Assume you have gone through the build process and produced the binary file \"dynomite\".\n   To run a one node dynomite cluster using the above config, you can run \n      \"dynomite -c dynomite.yml\".\n\n   If you want to start a 2 node cluster with those two yaml files, you can open \n   2 terminals and run:\n     Terminal 1: $ dynomite -c dynomite1.yml\n     Terminal 2: $ dynomite -c dynomite2.yml", 
            "title": "4.1 Setup and Installation"
        }, 
        {
            "location": "/4.1 Setup and Installation/#1-install-redis", 
            "text": "Please follow the instructions on this website to download and install Redis: http://redis.io/", 
            "title": "1. Install Redis"
        }, 
        {
            "location": "/4.1 Setup and Installation/#2-build-dynomite", 
            "text": "Please follow our  README.md  to compile and build Dynomite from source. Note that we only test Dynomite on Linux environments so we recommend compiling and building Dynomite on a Linux system.", 
            "title": "2. Build Dynomite"
        }, 
        {
            "location": "/4.1 Setup and Installation/#3-configuration", 
            "text": "These examples are for redis.  If you remove redis property, they will become memcached configured files.", 
            "title": "3. Configuration"
        }, 
        {
            "location": "/4.1 Setup and Installation/#a-one-node-cluster", 
            "text": "https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_single.yml", 
            "title": "a. One node cluster:"
        }, 
        {
            "location": "/4.1 Setup and Installation/#b-two-nodes-cluster-in-one-datacenter-with-2-racks", 
            "text": "https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_dc1_rack1.yml\n   https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_dc1_rack2.yml", 
            "title": "b. Two-nodes cluster in one datacenter with 2 racks"
        }, 
        {
            "location": "/4.1 Setup and Installation/#c-two-nodes-cluster-in-two-datacenters-each-having-one-rack", 
            "text": "https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_mul_dc1.yml\n   https://github.com/Netflix/dynomite/blob/master/conf/dyn_redis_mul_dc2.yml", 
            "title": "c. Two-nodes cluster in two datacenters each having one rack"
        }, 
        {
            "location": "/4.1 Setup and Installation/#d-6-node-cluster-in-one-datacenter-with-3-racks-each-having-2-nodes", 
            "text": "dyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '1383429731'\n     rack: rack-v1\n     data_store: 0   dyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '1383429731'\n     rack: rack-v0\n     data_store: 0   dyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '1383429731'\n     rack: rack-v2\n     data_store: 0   dyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '3530913378'\n     rack: rack-v1\n     data_store: 0   dyn_o_mite:\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     - ec2-23-123-53-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '3530913378'\n     rack: rack-v0\n     data_store: 0   dyn_o_mite:\n     auto_eject_hosts: true\n     datacenter: us-east-1\n     dyn_listen: 0.0.0.0:9101\n     dyn_seed_provider: simple_provider\n     dyn_seeds:\n     - ec2-54-111-131-14.compute-1.amazonaws.com:9101:rack-v1:us-east-1:1383429731\n     - ec2-54-162-161-10.compute-1.amazonaws.com:9101:rack-v0:us-east-1:1383429731\n     - ec2-54-191-177-122.compute-1.amazonaws.com:9101:rack-v2:us-east-1:1383429731\n     - ec2-54-181-51-12.compute-1.amazonaws.com:9101:rack-v1:us-east-1:3530913378\n     - ec2-54-129-121-134.compute-1.amazonaws.com:9101:rack-v0:us-east-1:3530913378\n     listen: 0.0.0.0:9102\n     servers:\n     - 127.0.0.1:6380:1\n     timeout: 30000\n     tokens: '3530913378'\n     rack: rack-v2\n     data_store: 0", 
            "title": "d. 6-node cluster in one datacenter with 3 racks, each having 2 nodes"
        }, 
        {
            "location": "/4.1 Setup and Installation/#4-start-the-clusters", 
            "text": "Assume you have gone through the build process and produced the binary file \"dynomite\".\n   To run a one node dynomite cluster using the above config, you can run \n      \"dynomite -c dynomite.yml\".\n\n   If you want to start a 2 node cluster with those two yaml files, you can open \n   2 terminals and run:\n     Terminal 1: $ dynomite -c dynomite1.yml\n     Terminal 2: $ dynomite -c dynomite2.yml", 
            "title": "4. Start the cluster(s)"
        }, 
        {
            "location": "/4.2 Configuration/", 
            "text": "Configuring Dynomite\n\n\nPlease refer to README.md for more details.\n\n\nToken Management\n\n\nDynomite is based on the Dynamo protocol where nodes serve keys that belong inside a token range. Differently from Cassandra, each Rack contains the full ring. Hence in RF=3 configuration the full ring is deployed in 3 Racks, whereas in RF=2 the full ring is deployed in 2 racks.\n\n\nTo create tokens you can use this script to generate \nYAMLS\n\n\nExemplar Configuration YAML files can be found in the \nconf directory", 
            "title": "4.2 Configuration"
        }, 
        {
            "location": "/4.2 Configuration/#configuring-dynomite", 
            "text": "Please refer to README.md for more details.", 
            "title": "Configuring Dynomite"
        }, 
        {
            "location": "/4.2 Configuration/#token-management", 
            "text": "Dynomite is based on the Dynamo protocol where nodes serve keys that belong inside a token range. Differently from Cassandra, each Rack contains the full ring. Hence in RF=3 configuration the full ring is deployed in 3 Racks, whereas in RF=2 the full ring is deployed in 2 racks.  To create tokens you can use this script to generate  YAMLS  Exemplar Configuration YAML files can be found in the  conf directory", 
            "title": "Token Management"
        }, 
        {
            "location": "/4.3 Debugging/", 
            "text": "You can use the admin port to increase the log level in order to identify the root cause of an issue. The following curl call will increase the log level. The default is 5. \n\n\n$ curl http://localhost:22222/loglevelup\n\n\n\nNote that the higher the log level, there is a performance hit from writing logs. This is though only valid if Dynomite receives multiple thousands of OPS. To decrease the log level.\n\n\n$ curl http://localhost:22222/logleveldown", 
            "title": "4.3 Debugging"
        }, 
        {
            "location": "/4.4 REST/", 
            "text": "Dynomite provides some functionalities through REST calls in the admin port \n$ curl http://localhost:22222/\narg\n. The following arguments have been implemented:\n\n\n\n\ninfo\n\n\nhelp\n\n\nping\n: performs a Redis inline ping to test Dynomite and storage\n\n\ndescribe\n\n\nloglevelup\n: increase the log level. More info in \nDebugging\n\n\nlogleveldown\n: decrease the log level. More info in \nDebugging\n\n\nhistoreset\n\n\ncluster_describe\n\n\nget_consistency\n: get the consistency level. More info in \nConsistency\n\n\nset_consistency\n: set the consistency level. More info in \nConsistency\n\n\nget_timeout_factor\n\n\nset_timeout_factor\n\n\n\n\nState\n\n\nOne can change the operation status of Dynomite dynamically by performing \n$ curl http://localhost:22222/state/\narg\n and one of the following arguments\n\n\n\n\nstandby\n: do not perform writes or reads\n\n\nwrites_only\n: perform writes only\n\n\nnormal\n: switch back to normal mode\n\n\nresuming", 
            "title": "4.4 REST"
        }, 
        {
            "location": "/4.4 REST/#state", 
            "text": "One can change the operation status of Dynomite dynamically by performing  $ curl http://localhost:22222/state/ arg  and one of the following arguments   standby : do not perform writes or reads  writes_only : perform writes only  normal : switch back to normal mode  resuming", 
            "title": "State"
        }, 
        {
            "location": "/4.5 Dynomite-Ecosystem/", 
            "text": "Dynomite is a standalone software. However, you can leverage:\n\n \nDyno\n client for Connection pooling of persistent connections, topology aware load balancing, application resilience by intelligently failing over and falling back, flexible retry policies etc. \n\n \nDynomite-manager\n a Dynomite sidecar for Discovery and Healthcheck, Node Configuration and Token Management for multi-region deployments, Dynomite/Redis cold bootstrap (warm up), object storage backups etc.\n\n \nGetting started with Dynomite and Dynomite-manager\n in CentOS.\n\n \nDynomite-cluster-checker\n a tool maintained by \nDiego Pacheco\n to validate if a Dynomite is setup properly.", 
            "title": "4.5 Dynomite Ecosystem"
        }, 
        {
            "location": "/5.0 How-to-contribute/", 
            "text": "Please see \nCONTRIBUTING\n.", 
            "title": "5.0 How to contribute"
        }, 
        {
            "location": "/6.0 Videos-and-Presentations/", 
            "text": "Videos\n\n\n\n\nNetflix Open Source Meetup Season 4 Episode 2 \nVideo\n\n\nRedisConf 2016 \nVideo\n and \nSlides", 
            "title": "6.0 Videos and Presentations"
        }, 
        {
            "location": "/6.0 Videos-and-Presentations/#videos", 
            "text": "Netflix Open Source Meetup Season 4 Episode 2  Video  RedisConf 2016  Video  and  Slides", 
            "title": "Videos"
        }, 
        {
            "location": "/7.0 FAQ/", 
            "text": "What is the relationship between a rack and a data center?\n\n\nA Dynomite cluster consists of a group of data centers and each data center has a number of racks.\nEach rack consists of a number of nodes.  Note that different racks can have different numbers of nodes.  Similarly, different data centers can have different numbers of racks.\n\n\nWhat is a replication factor?\n\n\nIn a Dynomite cluster, each rack contains a complete set of data. Replication factor is the number of complete data copies in a data center.  In other words, it is the number of racks in a data center.\n\n\nWhat are the currently supported datastore servers?\n\n\nRedis and Memcached, as well as other datastores through a protocol translation. We are currently experimenting with RocksDB, ForestDB and LMDB.\n\n\nWhy not just use Cassandra?\n\n\nCassandra is written in Java and at runtime, GC trigger can result in unexpected latencies in some cases.  Cassandra is fast for writing but in very high frequency writes, Cassandra node will get blown up very soon as the compaction can't catch up. Also during a compaction is running on a Cassandra node, read latency is also affected due to the extra CPU and disk loads. \n\n\nFurthermore, to achieve the same throughputs, and latencies as Dynomite, using Cassandra, you probably need to spend 15X-20X of the cost of a Dynomite cluster. \n\n\nIf you need more consistency, Cassandra at this point provides a better solution. However, Dynomite is going to provide different levels of consistencies very soon.  Stay tuned.\n\n\nWhy not use McRouter?\n\n\nDo you employ a Read-Modify-Write pattern? Do you love Redis and its API (set, list, hash)?  Do you want to simplify your cluster management leveraging a peer-to-peer system? Do you want to avoid an extra network hop that McRouter introduces? If you answered yes to these questions, then Dynomite is for you.\n\n\nWhy not use Redis cluster?\n\n\nDynomite is a peer-to-peer system.  Redis cluster is multi master/slave system.  You should choose whichever system works best for you.\n\n\nWhy not use Twemproxy?\n\n\nTwemproxy, as the name recalls, is just a proxy to route Memcached or Redis requests in a consistent hashing mechanism to a pool of Memcached or Redis servers. In a way, it is similar to McRouter but without any replications. You can consider Dynomite as a superset of Twemproxy's features. Indeed, Dynomite was originally a fork of Twemproxy before we went off to our own path.   \n\n\nWhat do we need to install before building Dynomite?\n\n\nYou need to install libevent, autoconf, libtool, and either Memcached or Redis\n\n\nHow can we monitor a Dynomite process?\n\n\nThere is an admin port that you can access with HTTP protocol to pull out statistical information from a running Dynomite process. \n\n\ncurl 'http://localhost:22222/info'\n\n\n\nIs this a fork from twemproxy?\n\n\nYes, it is. When we started we realized that a bunch of work already there in twemproxy to get us progress quickly.  However, since our design is so different from twemproxy's design, we eventually had to do what is best for our project while still retaining all twemproxy's commit histories, its copyright notices and also other open sources' copyright notices that twemproxy uses, e.g. the FreeBSD code from University of Berkeley.\n\n\nWhy is it written in C?\n\n\nWe were evaluating the pros and cons between a JVM language (Java/Scala), Go, or C/C++.  A JVM language will force us to worry about GC and we don't have a deterministic control over it.  Go seems to be a new and interesting language. However, at the time we explored it, it still had long GC issues in some cases and plus, not many detailed documents available on the internet to give us the confidence.  C/C++ give us a total control we can get the best possible performance to avoid the bottleneck issue in our layer.  We finally settled on C as we don't need many features from C++ and if we do, converting a C project to a C++ project should be straight forward.\n\n\nIs there a sidecar to manage a Dynomite cluster (e.g. like Priam for Cassandra)?\n\n\nYes, we have open sourced \nDynomite-manager\n. Dynomite-manager provides discovery and healthcheck, Dynomite/Redis cold bootstrap (warm up), monitoring and insight integration, multi-region Dynomite deployment via public IP, automated security group updates, object storage backups (AWS S3).\n\n\nWhat client can we use to connect to Dynomite?\n\n\nIf you are using a JVM language, we have our Java client, Dyno, for you.\nIt has many advanced features and you can read more about it at http://github.com/Netflix/dyno.\nOtherwise, you can use just any other Memcached or Redis client out there to connect but of course, you will not have those advanced features Dyno provide such as failover strategy, retries strategy, etc.", 
            "title": "7.0 FAQ"
        }, 
        {
            "location": "/7.0 FAQ/#what-is-the-relationship-between-a-rack-and-a-data-center", 
            "text": "A Dynomite cluster consists of a group of data centers and each data center has a number of racks.\nEach rack consists of a number of nodes.  Note that different racks can have different numbers of nodes.  Similarly, different data centers can have different numbers of racks.", 
            "title": "What is the relationship between a rack and a data center?"
        }, 
        {
            "location": "/7.0 FAQ/#what-is-a-replication-factor", 
            "text": "In a Dynomite cluster, each rack contains a complete set of data. Replication factor is the number of complete data copies in a data center.  In other words, it is the number of racks in a data center.", 
            "title": "What is a replication factor?"
        }, 
        {
            "location": "/7.0 FAQ/#what-are-the-currently-supported-datastore-servers", 
            "text": "Redis and Memcached, as well as other datastores through a protocol translation. We are currently experimenting with RocksDB, ForestDB and LMDB.", 
            "title": "What are the currently supported datastore servers?"
        }, 
        {
            "location": "/7.0 FAQ/#why-not-just-use-cassandra", 
            "text": "Cassandra is written in Java and at runtime, GC trigger can result in unexpected latencies in some cases.  Cassandra is fast for writing but in very high frequency writes, Cassandra node will get blown up very soon as the compaction can't catch up. Also during a compaction is running on a Cassandra node, read latency is also affected due to the extra CPU and disk loads.   Furthermore, to achieve the same throughputs, and latencies as Dynomite, using Cassandra, you probably need to spend 15X-20X of the cost of a Dynomite cluster.   If you need more consistency, Cassandra at this point provides a better solution. However, Dynomite is going to provide different levels of consistencies very soon.  Stay tuned.", 
            "title": "Why not just use Cassandra?"
        }, 
        {
            "location": "/7.0 FAQ/#why-not-use-mcrouter", 
            "text": "Do you employ a Read-Modify-Write pattern? Do you love Redis and its API (set, list, hash)?  Do you want to simplify your cluster management leveraging a peer-to-peer system? Do you want to avoid an extra network hop that McRouter introduces? If you answered yes to these questions, then Dynomite is for you.", 
            "title": "Why not use McRouter?"
        }, 
        {
            "location": "/7.0 FAQ/#why-not-use-redis-cluster", 
            "text": "Dynomite is a peer-to-peer system.  Redis cluster is multi master/slave system.  You should choose whichever system works best for you.", 
            "title": "Why not use Redis cluster?"
        }, 
        {
            "location": "/7.0 FAQ/#why-not-use-twemproxy", 
            "text": "Twemproxy, as the name recalls, is just a proxy to route Memcached or Redis requests in a consistent hashing mechanism to a pool of Memcached or Redis servers. In a way, it is similar to McRouter but without any replications. You can consider Dynomite as a superset of Twemproxy's features. Indeed, Dynomite was originally a fork of Twemproxy before we went off to our own path.", 
            "title": "Why not use Twemproxy?"
        }, 
        {
            "location": "/7.0 FAQ/#what-do-we-need-to-install-before-building-dynomite", 
            "text": "You need to install libevent, autoconf, libtool, and either Memcached or Redis", 
            "title": "What do we need to install before building Dynomite?"
        }, 
        {
            "location": "/7.0 FAQ/#how-can-we-monitor-a-dynomite-process", 
            "text": "There is an admin port that you can access with HTTP protocol to pull out statistical information from a running Dynomite process.   curl 'http://localhost:22222/info'", 
            "title": "How can we monitor a Dynomite process?"
        }, 
        {
            "location": "/7.0 FAQ/#is-this-a-fork-from-twemproxy", 
            "text": "Yes, it is. When we started we realized that a bunch of work already there in twemproxy to get us progress quickly.  However, since our design is so different from twemproxy's design, we eventually had to do what is best for our project while still retaining all twemproxy's commit histories, its copyright notices and also other open sources' copyright notices that twemproxy uses, e.g. the FreeBSD code from University of Berkeley.", 
            "title": "Is this a fork from twemproxy?"
        }, 
        {
            "location": "/7.0 FAQ/#why-is-it-written-in-c", 
            "text": "We were evaluating the pros and cons between a JVM language (Java/Scala), Go, or C/C++.  A JVM language will force us to worry about GC and we don't have a deterministic control over it.  Go seems to be a new and interesting language. However, at the time we explored it, it still had long GC issues in some cases and plus, not many detailed documents available on the internet to give us the confidence.  C/C++ give us a total control we can get the best possible performance to avoid the bottleneck issue in our layer.  We finally settled on C as we don't need many features from C++ and if we do, converting a C project to a C++ project should be straight forward.", 
            "title": "Why is it written in C?"
        }, 
        {
            "location": "/7.0 FAQ/#is-there-a-sidecar-to-manage-a-dynomite-cluster-eg-like-priam-for-cassandra", 
            "text": "Yes, we have open sourced  Dynomite-manager . Dynomite-manager provides discovery and healthcheck, Dynomite/Redis cold bootstrap (warm up), monitoring and insight integration, multi-region Dynomite deployment via public IP, automated security group updates, object storage backups (AWS S3).", 
            "title": "Is there a sidecar to manage a Dynomite cluster (e.g. like Priam for Cassandra)?"
        }, 
        {
            "location": "/7.0 FAQ/#what-client-can-we-use-to-connect-to-dynomite", 
            "text": "If you are using a JVM language, we have our Java client, Dyno, for you.\nIt has many advanced features and you can read more about it at http://github.com/Netflix/dyno.\nOtherwise, you can use just any other Memcached or Redis client out there to connect but of course, you will not have those advanced features Dyno provide such as failover strategy, retries strategy, etc.", 
            "title": "What client can we use to connect to Dynomite?"
        }, 
        {
            "location": "/_Sidebar/", 
            "text": "[[Home]]  \n\n\n[[Architecture]]\n\n\n[[Topology]]  \n\n\n[[Replication]] \n\n\n[[Consistency]]\n\n\n[[Membership protocols]]\n\n\n[[Warm up]]\n\n\n[[Performance]]\n\n\n[[Basic benchmarking]]\n\n\n[[Dynomite with Redis on AWS - Benchmarks]]\n\n\nGetting Started\n\n\nSetup and Installation\n \n\n\n[[Configuration]] \n\n\n[[Debugging]]\n\n\n[[REST]]\n\n\n[[Dynomite Ecosystem]]\n\n\n[[How to contribute]]\n\n\n[[Videos and Presentations]]\n\n\n[[FAQ]]", 
            "title": " Sidebar"
        }
    ]
}